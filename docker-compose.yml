version: "3"

services:
# UCI API
  uci-api-service:
    image: "samagragovernance/uci-apis:v2.4.3"
    env_file: .env
    ports:
      - 9999:3002
    restart: always
    depends_on:
      aggregate-server:
        condition: service_started
      fusionauth:
        condition: service_healthy
      uci-api-scheduler-db:
        condition: service_started
    healthcheck:
      test:
        [ "CMD-SHELL", "curl -f http://localhost:3002/admin/health/ping --header 'admin-token: ${ADMIN_TOKEN}' || exit 1" ]
      interval: 3s
      timeout: 5s
      retries: 5
  
  uci-api-db:
    image: postgres:12
    env_file: .env
    restart: always
    environment:
      - POSTGRES_PASSWORD=${UCI_API_POSTGRES_PASSWORD}
      - POSTGRES_USER=${UCI_API_POSTGRES_USER}
      - POSTGRES_DB=${UCI_API_POSTGRES_DB}
    volumes:
      - ./data/pgdata-uci-api-db:/var/lib/postgresql/data
      - ./dump.sql:/docker-entrypoint-initdb.d/dump.sql

  uci-api-db-gql:
    image: hasura/graphql-engine:latest
    environment:
      HASURA_GRAPHQL_DATABASE_URL: postgres://postgresql:${UCI_API_POSTGRES_PASSWORD}@${POSTGRES_HOST}:${UCI_API_DB_PORT}/${UCI_API_POSTGRES_DB}
      HASURA_GRAPHQL_ENABLE_CONSOLE: "true"
      HASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, http-log, webhook-log, websocket-log, query-log
      HASURA_GRAPHQL_ADMIN_SECRET: ${UCI_API_DB_GRAPHQL_SECRET}
    ports:
      - "15003:8080"
    restart: always

  uci-api-scheduler-db:
    image: redis:6.2-alpine
    command: ["redis-server", "--appendonly", "yes"]
    restart: always

# UCI PWA Adapter

  uci-transport-socket:
    image: samagragovernance/uci-transport-socket:v2.1.11
    env_file: .env
    environment:
      REDIS_HOST: ${TRANSPORT_SOCKET_CACHE_HOST}
      REDIS_PORT: ${TRANSPORT_SOCKET_CACHE_PORT}
      CACHE_STORE: ${CACHE_STORE}
      SERVER_PORT: 3005
      ADAPTER_URL: "http://inbound:8085/pwa/web"
    ports:
      - 3005:3005
    depends_on:
      outbound:
        condition: service_healthy
    restart: always
    healthcheck:
      test:
        [ "CMD-SHELL", "curl -f http://localhost:3005/health || exit 1" ]
      interval: 3s
      timeout: 5s
      retries: 5

  uci-web-channel:
    restart: always
    build:
      context: ./uci-web-channel
    env_file: .env
    ports:
    - "9098:3000"

  uci-admin:
    restart: always
    build: 
      context: ./uci-admin/
      args:
        NG_APP_url: ${NG_APP_url}
        NG_APP_nl_url: ${NG_APP_nl_url}
        NG_APP_nl_login_url: ${NG_APP_nl_login_url}
        NG_APP_nl_login_token: ${NG_APP_nl_login_token}
        NG_APP_nl_application_id: ${NG_APP_nl_application_id}
        NG_APP_blobUrl: ${NG_APP_blobUrl}
        NG_APP_botPhoneNumber: ${NG_APP_botPhoneNumber}
        NG_APP_adapterId: ${NG_APP_adapterId}
        NG_APP_broadcastAdapterId: ${NG_APP_broadcastAdapterId}
        NG_APP_userId: ${NG_APP_userId}
        NG_APP_orgId: ${NG_APP_orgId}
        NG_APP_token: ${NG_APP_token}
    image: uci-admin
    ports:
    - "9097:80"


  cache:
    image: redis:latest
    restart: always
    command: ['redis-server', '--appendonly', 'yes']
    hostname: redis
    volumes:
      - ./data/redis-data:/data
      - ./redis.conf:/usr/local/etc/redis/redis.conf

# Kafka

  zookeeper:
    image: zookeeper:3.9
    restart: on-failure

  kafka:
    image: bitnami/kafka:2.8.1-debian-10-r99
    restart: on-failure
    environment:
        KAFKA_BROKER_ID: 1001
        KAFKA_CFG_RESERVED_BROKER_MAX_ID: 1001
        KAFKA_CFG_LISTENERS: PLAINTEXT://:9092
        KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
        KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
        ALLOW_PLAINTEXT_LISTENER: 'true'
    depends_on:
      zookeeper:
        condition: service_started
    healthcheck:
      test: ["CMD", "kafka-topics.sh", "--list", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 5s
      retries: 3

  schema-registry:
    image: confluentinc/cp-schema-registry
    restart: always
    depends_on:
      - kafka
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka:9092'
      SCHEMA_REGISTRY_HOST_NAME: 'schema-registry'
      SCHEMA_REGISTRY_LISTENERS: 'http://0.0.0.0:8085'
      SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: 'INFO'

  connect:
    image: confluentinc/cp-kafka-connect
    restart: always
    depends_on:
      - kafka
      - schema-registry
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'kafka:9092'
      CONNECT_REST_PORT: '8083'
      CONNECT_REST_LISTENERS: 'http://0.0.0.0:8083'
      CONNECT_REST_ADVERTISED_HOST_NAME: 'connect'
      CONNECT_CONFIG_STORAGE_TOPIC: '__connect-config'
      CONNECT_OFFSET_STORAGE_TOPIC: '__connect-offsets'
      CONNECT_STATUS_STORAGE_TOPIC: '__connect-status'
      CONNECT_GROUP_ID: 'kafka-connect'
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: 'true'
      CONNECT_KEY_CONVERTER: 'io.confluent.connect.avro.AvroConverter'
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8085'
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: 'true'
      CONNECT_VALUE_CONVERTER: 'io.confluent.connect.avro.AvroConverter'
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8085'
      CONNECT_INTERNAL_KEY_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_INTERNAL_VALUE_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: '1'
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: '1'
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: '1'
      CONNECT_PLUGIN_PATH: ' /usr/share/java/'

# UCI Core
  inbound:
    image: "samagragovernance/inbound:${CURRENT_VERSION}"
    env_file: .env
    ports:
      - "0.0.0.0:${INBOUND_EXTERNAL_PORT}:${INBOUND_INTERNAL_PORT}"
    restart: always
    depends_on:
      uci-api-service:
        condition: service_healthy
      kafka:
        condition: service_healthy
      uci-api-scheduler-db:
        condition: service_started
      fusionauth:
        condition: service_healthy
    healthcheck:
      test:
        [ "CMD-SHELL", "curl -f http://localhost:${INBOUND_INTERNAL_PORT}/health || exit 1" ]
      interval: 8s
      timeout: 5s
      retries: 5

  orchestrator:
    image: "samagragovernance/orchestrator:${CURRENT_VERSION}"
    env_file: .env
    ports:
      - "${ORCHESTRATOR_INTERNAL_PORT}:${ORCHESTRATOR_INTERNAL_PORT}"
    restart: always
    depends_on:
      inbound:
        condition: service_healthy
    healthcheck:
      test:
        [ "CMD-SHELL", "curl -f http://localhost:${ORCHESTRATOR_INTERNAL_PORT}/health || exit 1" ]
      interval: 8s
      timeout: 5s
      retries: 5

  transformer:
    image: "samagragovernance/transformer:${CURRENT_VERSION}"
    env_file: .env
    ports:
      - "${TRANSFORMER_INTERNAL_PORT}:${TRANSFORMER_INTERNAL_PORT}"
    restart: always
    depends_on:
      orchestrator:
        condition: service_healthy
      aggregate-server:
        condition: service_started
    healthcheck:
      test:
        [ "CMD-SHELL", "curl -f http://localhost:${TRANSFORMER_INTERNAL_PORT}/health || exit 1" ]
      interval: 15s
      timeout: 5s
      retries: 8

  outbound:
    image: "samagragovernance/outbound:${CURRENT_VERSION}"
    env_file: .env
    ports:
      - "${OUTBOUND_INTERNAL_PORT}:${OUTBOUND_INTERNAL_PORT}"
    restart: always
    depends_on:
      transformer:
        condition: service_healthy
      broadcast-transformer:
        condition: service_healthy
    healthcheck:
      test:
        [ "CMD-SHELL", "curl -f http://localhost:${OUTBOUND_INTERNAL_PORT}/health || exit 1" ]
      interval: 8s
      timeout: 5s
      retries: 5

  broadcast-transformer:
    image: "samagragovernance/broadcast-transformer:${CURRENT_VERSION}"
    env_file: .env
    ports:
      - "${BROADCAST_TRANSFORMER_INTERNAL_PORT}:${BROADCAST_TRANSFORMER_INTERNAL_PORT}"
    restart: always
    depends_on:
      orchestrator:
        condition: service_healthy
    healthcheck:
      test:
        [ "CMD-SHELL", "curl -f http://localhost:${BROADCAST_TRANSFORMER_INTERNAL_PORT}/health || exit 1" ]
      interval: 8s
      timeout: 5s
      retries: 5

  cdac:
    image: samagragovernance/cdac-service:4.10.3
    restart: always
    env_file: .env-cdac

  cass:
    image: cassandra:latest
    ports:
      - 7000:7000
      - 7001:7001
      - 7199:7199
      - 9041:9042
      - 9160:9160
    environment:
      - JVM_OPTS=-Xmx8G -Xms1G -Xss512k
      - MAX_HEAP_SIZE=8092M
      - HEAP_NEWSIZE=8092M
      - CASSANDRA_RPC_ADDRESS=0.0.0.0
      - CASSANDRA_PASSWORD=${CASSANDRA_PASSWORD}
    restart: always
    healthcheck:
      test: ["CMD", "cqlsh", "-e", "describe keyspaces"]
      interval: 5s
      timeout: 5s
      retries: 5
    volumes:
      - ./data/cd-data:/var/lib/cassandra
      - ./cassandra.yaml:/opt/cassandra/conf/cassandra.yaml
      - ./init-scripts/cassandra.cql:/docker-entrypoint-initdb.d/cassandra.cql
  formsdb:
    restart: always
    image: postgres:9.6
    environment:
      POSTGRES_PASSWORD: ${FORMS_DB_PASSWORD}
      POSTGRES_USER: ${FORMS_DB_USERNAME}
      POSTGRES_DB: ${FORMS_DB_NAME}
    volumes:
      - ./data/formsdb-data:/var/lib/postgresql/data

  graphql-formsdb:
    image: hasura/graphql-engine:latest
    ports:
      - "15002:8080"
    environment:
      HASURA_GRAPHQL_DATABASE_URL: postgres://postgresql:${FORMS_DB_PASSWORD}@${FORMS_DB_HOST}:${FORMS_DB_PORT}/formsdb
      HASURA_GRAPHQL_ENABLE_CONSOLE: "true" # set to "false" to disable console
      HASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, http-log, webhook-log, websocket-log, query-log
      HASURA_GRAPHQL_ADMIN_SECRET: ${FORMS_DB_GRAPHQL_SECRET}
    restart: always

# ODK      
  aggregate-db:
    restart: always
    image: postgres:9.6-alpine
    environment:
      - POSTGRES_PASSWORD=${ODK_POSTGRES_PASSWORD}
      - POSTGRES_USER=${ODK_POSTGRES_USER}
      - POSTGRES_DB=${ODK_POSTGRES_DB}
    volumes:
      - ./odk-aggregate/odk/initdb:/docker-entrypoint-initdb.d
      - ./data/pgdata-odk:/var/lib/postgresql/data

  wait_for_db:
    restart: always
    image: dadarek/wait-for-dependencies
    depends_on:
      - aggregate-db
    command: aggregate-db:5432

  aggregate-server:
    image: tomcat:alpine
    environment:
      - DB_USERNAME=${ODK_POSTGRES_USER}
      - DB_SCHEMA=public
      - DB_PASSWORD=${ODK_POSTGRES_PASSWORD}
      - DB_NAME=${ODK_POSTGRES_DB}
      - DB_PORT=5432
      - AGGREGATE_HOST=
      - DB_URL=jdbc:postgresql://${ODK_POSTGRES_HOST}/${ODK_POSTGRES_DB}?user=${ODK_POSTGRES_USER}&password=${ODK_POSTGRES_PASSWORD}
    ports:
      - 127.0.0.1:8081:8080
    volumes:
      - ./odk-aggregate/odk/webapps:/usr/local/tomcat/webapps
    restart: always

# Fusion Auth

  fa-search:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    env_file:
      - .env
    environment:
      cluster.name: fusionauth
      bootstrap.memory_lock: "true"
      discovery.type: single-node
      ES_JAVA_OPTS: ${ES_JAVA_OPTS}
    healthcheck:
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - fa-es-data:/usr/share/elasticsearch/data

  fusionauth:
    image: fusionauth/fusionauth-app:latest
    environment:
      DATABASE_URL: jdbc:postgresql://${FUSIONAUTH_DB_URL}/fusionauth
      DATABASE_ROOT_USERNAME: ${FUSIONAUTH_POSTGRES_USER}
      DATABASE_ROOT_PASSWORD: ${FUSIONAUTH_POSTGRES_PASSWORD}
      DATABASE_USERNAME: ${FUSIONAUTH_DATABASE_USERNAME}
      DATABASE_PASSWORD: ${FUSIONAUTH_DATABASE_PASSWORD}
      FUSIONAUTH_SEARCH_ENGINE_TYPE: elasticsearch
      FUSIONAUTH_SEARCH_SERVERS: http://elastic:${ELASTIC_PASSWORD}@${ES_HOST}
      FUSIONAUTH_URL: http://fusionauth:9011
      FUSIONAUTH_API_KEY: ${FUSIONAUTH_API_KEY}
      FUSIONAUTH_APP_KICKSTART_FILE: ${FUSIONAUTH_APP_KICKSTART_FILE}
      FUSIONAUTH_ADMIN_EMAIL: ${FUSIONAUTH_ADMIN_EMAIL}
      FUSIONAUTH_ADMIN_PASSWORD: ${FUSIONAUTH_ADMIN_PASSWORD}
    ports:
      - 127.0.0.1:9023:9011
    volumes:
      - ./kickstart:/usr/local/fusionauth/kickstart
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9011/"]
      interval: 5s
      timeout: 5s
      retries: 3
    restart: always

  fa-db:
    image: postgres:12
    restart: always
    environment:
      - POSTGRES_USER=${FUSIONAUTH_POSTGRES_USER}
      - POSTGRES_PASSWORD=${FUSIONAUTH_POSTGRES_PASSWORD}
    volumes:
      - ./data/pgdata-fa-db:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
  
  vault:
    image: vault:1.10.3
    restart: always
    volumes:
      - ./vault/vault.json:/vault/config/vault.json
      - ./data/vault-data:/vault/file
    environment:
      - VAULT_ADDR=http://0.0.0.0:8200
      - VAULT_API_ADDR=http://0.0.0.0:8200
      - VAULT_ADDRESS=http://0.0.0.0:8200
    cap_add:
      - IPC_LOCK
    command: vault server -config=/vault/config/vault.json
    ports:
      - 8200:8200

  uci-minio:
    image: quay.io/minio/minio
    ports:
      - "9000:9000"
      - "9001:9001"
    env_file: env.dev/minio.env
    command: server /data --console-address ":9001"
    environment:
      MINIO_IDENTITY_OPENID_CLIENT_ID: ${FUSIONAUTH_APPLICATION_ID}
      MINIO_IDENTITY_OPENID_CLIENT_SECRET: ${FUSIONAUTH_CLIENT_SECRET}
    depends_on:
      - fusionauth

  akhq:
    image: tchiotludo/akhq
    ports:
      - "127.0.0.1:18080:8080"
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          clients-defaults:
            consumer:
              properties:
                default.api.timeout.ms: 15000
          security:
            basic-auth:
              - username: admin
                password: admin
                groups: 
                  - admin
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: "kafka:9092"
    restart: always
    cpus: "1"
    mem_limit: "1gb"

  templater:
    image: samagragovernance/templater:latest
    env_file: .env
    ports:
      - '0.0.0.0:${TEMPLATER_PORT}:3000'
    restart: always
    environment:
      DATABASE_URL: ${TEMPLATER_DATABASE_URL}

volumes:
  fa-es-data:
